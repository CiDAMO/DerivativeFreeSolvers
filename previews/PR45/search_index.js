var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [DerivativeFreeSolvers]","category":"page"},{"location":"reference/#DerivativeFreeSolvers.coordinate_search-Tuple{NLPModels.AbstractNLPModel}","page":"Reference","title":"DerivativeFreeSolvers.coordinate_search","text":"coordinate_search(nlp)\n\nThe coordinate search is a derivative free optimization method. It minimizes a certain function f by walking on a grid and taking a step towards smaller function values. When it can no longer find any smaller values, it reduces the grid by a factor of β, repeating the process.\n\n\n\n\n\n","category":"method"},{"location":"reference/#DerivativeFreeSolvers.mads-Tuple{NLPModels.AbstractNLPModel}","page":"Reference","title":"DerivativeFreeSolvers.mads","text":"mads(nlp)\n\nMADS is implemented following the description of\n\nMark A. Abramson, Charles Audet, J. E. Dennis Jr., and Sébastien Le Digabel.\nOrthoMADS: A Deterministic MADS Instance with Orthogonal Directions.\nSIAM Journal on Optimization, 2009, Vol. 20, No. 2, pp. 948-966,\n\nBut using random vectors instead of Halton vectors, as suggested by Francisco Sobral.\n\n\n\n\n\n","category":"method"},{"location":"reference/#DerivativeFreeSolvers.nelder_mead-Tuple{NLPModels.AbstractNLPModel}","page":"Reference","title":"DerivativeFreeSolvers.nelder_mead","text":"nelder_mead(nlp)\n\nThis implementation follows the algorithm described in chapter 9 of [1]. The Oriented Restart follows [2]. [1] Numerical Optimization (Jorge Nocedal and Stephen J. Wright), Springer, 2006. [2] C. T. Kelley. Detection and remediation of stagnation in the nelder–mead algorithm using a sufficient decrease condition. SIAM J. on Optimization, 1999.\n\n\n\n\n\n","category":"method"},{"location":"#DerivativeFreeSolvers.jl","page":"Home","title":"DerivativeFreeSolvers.jl","text":"","category":"section"},{"location":"tutorial/#DerivativeFreeSolvers.jl-Tutorial","page":"Tutorial","title":"DerivativeFreeSolvers.jl Tutorial","text":"","category":"section"}]
}
